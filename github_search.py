#!/usr/bin/env python3
"""
GitHub ä»“åº“æœç´¢å‘½ä»¤è¡Œå·¥å…· (å¸¦åˆ†é¡µ)
ç”¨æ³•ï¼špython github_search.py <å…³é”®è¯> [--page <é¡µç >]
ç¤ºä¾‹ï¼š
  python github_search.py python machine learning
  python github_search.py python --page 2
"""

import requests
import sys
import argparse

# GitHub API åŸºç¡€ URL
GITHUB_API_URL = "https://api.github.com/search/repositories"

def truncate_text(text, max_length=100):
    """
    å¦‚æœæ–‡æœ¬é•¿åº¦è¶…è¿‡ max_lengthï¼Œåˆ™æˆªæ–­å¹¶åœ¨æœ«å°¾æ·»åŠ  '...'
    """
    if len(text) > max_length:
        return text[:max_length] + "..."
    return text

def search_github_repos(query, page=1, per_page=10):
    """
    ä½¿ç”¨ GitHub API æœç´¢ä»“åº“ï¼Œæ”¯æŒåˆ†é¡µ
    """
    params = {
        'q': query,
        'sort': 'stars',
        'order': 'desc',
        'page': page,
        'per_page': per_page
    }

    headers = {
        'Accept': 'application/vnd.github.v3+json',
        'User-Agent': 'GitHub-Search-CLI'
    }

    try:
        response = requests.get(GITHUB_API_URL, params=params, headers=headers, timeout=10)
        response.raise_for_status()
    except requests.exceptions.RequestException as e:
        print(f"âŒ è¯·æ±‚å¤±è´¥: {e}")
        sys.exit(1)

    data = response.json()
    repos = data.get('items', [])
    total_count = data.get('total_count', 0)
    total_pages = (total_count + per_page - 1) // per_page  # å‘ä¸Šå–æ•´

    if not repos:
        print(f"ğŸ” æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä»“åº“ã€‚")
        return

    # è®¡ç®—å½“å‰é¡µçš„èµ·å§‹å’Œç»“æŸç´¢å¼•
    start_index = (page - 1) * per_page + 1
    end_index = min(page * per_page, total_count)

    print(f"\nğŸ‰ æ‰¾åˆ° {total_count} ä¸ªä»“åº“ï¼Œæ˜¾ç¤ºç¬¬ {start_index}-{end_index} ä¸ª (å…± {total_pages} é¡µï¼Œå½“å‰ç¬¬ {page} é¡µ)ï¼š\n")
    print("-" * 80)

    for repo in repos:
        name = repo.get('full_name', 'N/A')
        description = repo.get('description', 'æ— æè¿°')
        # ğŸ‘‡ ä½¿ç”¨æˆ‘ä»¬ä¹‹å‰åŠ çš„æˆªæ–­åŠŸèƒ½
        description = truncate_text(description, 100)
        stars = repo.get('stargazers_count', 0)
        html_url = repo.get('html_url', 'N/A')
        owner = repo.get('owner', {}).get('login', 'N/A')

        print(f"ğŸŒŸ {name}")
        print(f"   ğŸ‘¤ ä½œè€…: {owner}")
        print(f"   â­ æ˜Ÿæ ‡: {stars}")
        print(f"   ğŸ“ æè¿°: {description}")
        print(f"   ğŸ”— é“¾æ¥: {html_url}")
        print("-" * 80)

    # å¦‚æœå½“å‰é¡µä¸æ˜¯æœ€åä¸€é¡µï¼Œæç¤ºç”¨æˆ·å¯ä»¥ç¿»é¡µ
    if page < total_pages:
        print(f"\nğŸ’¡ æç¤ºï¼šä½¿ç”¨ --page {page + 1} æŸ¥çœ‹ä¸‹ä¸€é¡µ")

def main():
    parser = argparse.ArgumentParser(description="åœ¨ GitHub ä¸Šæœç´¢ä»“åº“ (æ”¯æŒåˆ†é¡µ)")
    parser.add_argument('query', nargs='+', help='æœç´¢å…³é”®è¯ï¼ˆå¦‚ï¼špython machine learningï¼‰')
    parser.add_argument('--page', type=int, default=1, help='è¦æ˜¾ç¤ºçš„é¡µç  (é»˜è®¤ä¸º1)')
    args = parser.parse_args()

    # å°†å¤šä¸ªå‚æ•°æ‹¼æ¥æˆä¸€ä¸ªæœç´¢å­—ç¬¦ä¸²
    query = ' '.join(args.query)
    page = args.page

    # ç®€å•çš„é¡µç éªŒè¯
    if page < 1:
        print("âŒ é¡µç å¿…é¡»å¤§äº0")
        sys.exit(1)

    print(f"ğŸ” æ­£åœ¨æœç´¢: '{query}'...")
    search_github_repos(query, page=page)

if __name__ == "__main__":
    main()
